{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a4eabdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 00:05:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setMaster('spark://master:7077').setAppName('ItemSets')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a1e4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.app.submitTime', '1670457958325'),\n",
       " ('spark.master', 'spark://master:7077'),\n",
       " ('spark.driver.host', 'master'),\n",
       " ('spark.app.name', 'ItemSets'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.port', '38905'),\n",
       " ('spark.app.startTime', '1670457958608'),\n",
       " ('spark.app.id', 'app-20221208000601-0005'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.setLogLevel('ERROR')\n",
    "sc.version\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b905cc2a",
   "metadata": {},
   "source": [
    "Load dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bdd465e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileRDD = (\n",
    "    sc\n",
    "    .textFile('data/itemsets.txt', 8) \n",
    "    .map(lambda x: x.strip())\n",
    ")\n",
    "fileRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cec7952",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRDD = (\n",
    "    fileRDD\n",
    "    .map(lambda x: sorted([int(i) for i in x.split(' ')]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e4e4180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "support = 100 # support threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2886f8db",
   "metadata": {},
   "source": [
    "# 1. Find the products which are frequently browsed together by using the A-priori algorithm. Find itemsets of size 1, 2 and 3.  \n",
    "For itemsets size 1, count the number of appearances of each item in the dataset and filter with the support threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "016fa5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797 itemsets of size 1\n",
      "[((35,), 1984), ((283,), 4082), ((883,), 4902), ((947,), 3690), ((979,), 132)]\n",
      "Time taken: 4.592349290847778s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "itemset1 = (\n",
    "    dataRDD\n",
    "    .flatMap(lambda x: x)\n",
    "    .map(lambda x: ((x,), 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .filter(lambda x: x[1] >= support)\n",
    ")\n",
    "print(f'{itemset1.count()} itemsets of size 1')\n",
    "print(itemset1.take(5))\n",
    "print(f'Time taken: {time.time() - start_time}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014186df",
   "metadata": {},
   "source": [
    "For itemsets size 2, we first generate the candidates of size 2 from frequent itemsets size 1 above  \n",
    "Use `cartersian` to generate the candidates from rdd and filter out duplications  \n",
    "Then we count the number of appearances of each candidate in the origin dataset and filter by the support threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c5bd55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8831 itemsets of size 2\n",
      "[((52, 538), 247), ((52, 730), 424), ((274, 328), 265), ((274, 368), 156), ((274, 448), 290)]\n",
      "Time taken: 26.485960483551025s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "c_2 = itemset1.map(lambda x: (x[0][0]))\n",
    "c_2 = set(\n",
    "    c_2\n",
    "    .cartesian(c_2)\n",
    "    .filter(lambda x: x[0] < x[1])\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "itemset2 = (\n",
    "    dataRDD\n",
    "    .flatMap(\n",
    "        lambda x: [tuple(sorted((x[i], x[j]))) for i in range(len(x)) for j in range(i+1, len(x))]\n",
    "    )\n",
    "    .map(lambda x: (x, 1) if x in c_2 else (x, 0))\n",
    "    .filter(lambda x: x[1] > 0)\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .filter(lambda x: x[1] >= support)\n",
    ")\n",
    "\n",
    "print(f'{itemset2.count()} itemsets of size 2')\n",
    "print(itemset2.take(5))\n",
    "print(f'Time taken: {time.time() - start_time}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d77f6bd",
   "metadata": {},
   "source": [
    "For itemsets size 3, we generate the candidates of size 3 from the frequent itemsets size 2 using `cartersian`  \n",
    "Then we count the number of appearances similar the previous block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b49b41bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7130 itemsets of size 3\n",
      "[((140, 414, 935), 176), ((14, 160, 887), 174), ((161, 558, 774), 224), ((336, 385, 738), 135), ((385, 422, 606), 130)]\n",
      "Time taken: 194.64307403564453s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "c_3 = (\n",
    "    itemset2\n",
    "    .map(lambda x: x[0])\n",
    ")\n",
    "c_3 = set(\n",
    "    c_3\n",
    "    .cartesian(c_3)\n",
    "    .map(lambda x: tuple(sorted(list(set(x[0]) | set(x[1])))))\n",
    "    .filter(lambda x: len(x) == 3)\n",
    "    .distinct()\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "itemset3 = (\n",
    "    dataRDD\n",
    "    .flatMap(\n",
    "        lambda x: [tuple(sorted((x[i], x[j], x[k]))) for i in range(len(x)) for j in range(i+1, len(x)) for k in range(j+1, len(x))]\n",
    "    )\n",
    "    .filter(lambda x: x[0] < x[1] and x[1] < x[2])\n",
    "    .map(lambda x: (x, 1) if x in c_3 else (x, 0))\n",
    "    .filter(lambda x: x[1] > 0)\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .filter(lambda x: x[1] >= support)\n",
    ")\n",
    "print(f'{itemset3.count()} itemsets of size 3')\n",
    "print(itemset3.take(5))\n",
    "print(f'Time taken: {time.time() - start_time}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058a80aa",
   "metadata": {},
   "source": [
    "# 2. Identify item triples (X, Y, Z) such that the support of {X, Y, Z} is at least 100. For all such triples, compute the confidence scores of the corresponding association rules:  \n",
    "(X, Y ) → Z,  \n",
    "(X, Z) → Y,  \n",
    "(Y, Z) → X.  \n",
    "Sort the rules in decreasing order of confidence scores and list the top 5 rules.  \n",
    "\n",
    "The condifence scores of the association rules (X, Y) → Z is: `sup(X, Y, Z) / sup(X, Y)`  \n",
    "First we find the support for itemsets size 2, store it into a map for easy to access  \n",
    "Then we use `flatMap` to map each itemset size 3 `(X, Y, Z)` into 3 different association rules `(X, Y ) → Z`, `(X, Z) → Y`, `(Y, Z) → X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e85c9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 rules: [((120, 124, 205), 243), ((120, 124, 581), 240), ((124, 205, 834), 244), ((124, 581, 834), 244), ((262, 294, 853), 347)]\n",
      "Time taken: 6.500771760940552s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "itemset2_map = {}\n",
    "for item in itemset2.collect():\n",
    "    itemset2_map[item[0]] = item[1]\n",
    "\n",
    "associationRule = (\n",
    "    itemset3\n",
    "    .flatMap(\n",
    "        lambda x: [\n",
    "            ((x[0][0], x[0][1]) , x[0][2], x[1] / itemset2_map[tuple(sorted((x[0][0], x[0][1])))]),\n",
    "            ((x[0][1], x[0][2]) , x[0][0], x[1] / itemset2_map[tuple(sorted((x[0][1], x[0][2])))]),\n",
    "            ((x[0][0], x[0][2]) , x[0][1], x[1] / itemset2_map[tuple(sorted((x[0][0], x[0][2])))])\n",
    "        ]\n",
    "    )\n",
    "    .sortBy(lambda x: -x[2])\n",
    ")\n",
    "associationRule.take(5)\n",
    "print(f'Top 5 rules: {itemset3.take(5)}')\n",
    "print(f'Time taken: {time.time() - start_time}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f0042",
   "metadata": {},
   "source": [
    "# 3. Use SON algorithm to identify frequent itemsets with the same value of support.\n",
    "Use mapPartition to split the dataset into smaller partition and find the frequent itemsets in each partition. Since we divide into 8 partitions, then the support threshold in each partition should be `100/8`. \n",
    "Then all the frequent itemsets from all partitions are gather. For each itemset from the partition, we count its number of appearances and compare with the support threshold (`100`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b29caf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797 itemsets of size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8831 itemsets of size 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7130 itemsets of size 3\n",
      "Total time taken: 159.12080717086792s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "def localItemset(partition):\n",
    "    result = []\n",
    "    support = round(100 / 8)\n",
    "    data = [x for x in partition]\n",
    "    \n",
    "    itemset1 = {}\n",
    "    for line in data:\n",
    "        for item in line:\n",
    "            x = int(item)\n",
    "            itemset1[x] = itemset1.get(x, 0) + 1\n",
    "    itemset1 = [(k,) for k, v in itemset1.items() if v >= support]\n",
    "    \n",
    "    c_2 = set([\n",
    "        tuple(sorted((itemset1[i][0], itemset1[j][0])))\n",
    "        for i in range(len(itemset1))\n",
    "        for j in range(i+1, len(itemset1))\n",
    "    ])\n",
    "\n",
    "    itemset2 = {}\n",
    "    for line in data:\n",
    "        for i in range(len(line)):\n",
    "            for j in range(i+1, len(line)):\n",
    "                x = tuple(sorted((line[i], line[j])))\n",
    "                if x in c_2:\n",
    "                    itemset2[x] = itemset2.get(x, 0) + 1\n",
    "    itemset2 = [k for k, v in itemset2.items() if v >= support]\n",
    "\n",
    "    c_3 = []\n",
    "    for i in range(len(itemset2)):\n",
    "        for j in range(i+1, len(itemset2)):\n",
    "            x = set(itemset2[i]) | set(itemset2[j])\n",
    "            if len(x) == 3:\n",
    "                c_3.append(tuple(sorted(list(x))))\n",
    "    c_3 = set(c_3)\n",
    "    itemset3 = {}\n",
    "    for line in data:\n",
    "        for i in range(len(line)):\n",
    "            for j in range(i+1, len(line)):\n",
    "                for k in range(j+1, len(line)):\n",
    "                    x = tuple(sorted((line[i], line[j], line[k])))\n",
    "                    if x in c_3:\n",
    "                        itemset3[x] = itemset3.get(x, 0) + 1\n",
    "    itemset3 = [k for k, v in itemset3.items() if v >= support]\n",
    "    \n",
    "    return [itemset1, itemset2, itemset3]\n",
    "\n",
    "sonResult = dataRDD.mapPartitions(localItemset).collect()\n",
    "\n",
    "c_1 = set()\n",
    "c_2 = set()\n",
    "c_3 = set()\n",
    "for line in sonResult:\n",
    "    for x in line:\n",
    "        if len(x) == 1:\n",
    "            c_1.add(x)\n",
    "        if len(x) == 2:\n",
    "            c_2.add(x)\n",
    "        if len(x) == 3:\n",
    "            c_3.add(x)\n",
    "            \n",
    "itemset1 = (\n",
    "    dataRDD\n",
    "    .flatMap(\n",
    "        lambda x: x\n",
    "    )\n",
    "    .map(lambda x: ((x,), 1) if (x,) in c_1 else ((x,), 0))\n",
    "    .filter(lambda x: x[1] > 0)\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .filter(lambda x: x[1] >= support)\n",
    ")\n",
    "print(f'{itemset1.count()} itemsets of size 1')\n",
    "\n",
    "itemset2 = (\n",
    "    dataRDD\n",
    "    .flatMap(\n",
    "        lambda x: [(x[i], x[j]) for i in range(len(x)) for j in range(i+1, len(x))]\n",
    "    )\n",
    "    .filter(lambda x: x[0] < x[1])\n",
    "    .map(lambda x: (x, 1) if x in c_2 else (x, 0))\n",
    "    .filter(lambda x: x[1] > 0)\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .filter(lambda x: x[1] >= support)\n",
    ")\n",
    "print(f'{itemset2.count()} itemsets of size 2')\n",
    "\n",
    "itemset3 = (\n",
    "    dataRDD\n",
    "    .flatMap(\n",
    "        lambda x: [(x[i], x[j], x[k]) for i in range(len(x)) for j in range(i+1, len(x)) for k in range(j+1, len(x))]\n",
    "    )\n",
    "    .filter(lambda x: x[0] < x[1] and x[1] < x[2])\n",
    "    .map(lambda x: (x, 1) if x in c_3 else (x, 0))\n",
    "    .filter(lambda x: x[1] > 0)\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .filter(lambda x: x[1] >= support)\n",
    ")\n",
    "print(f'{itemset3.count()} itemsets of size 3')\n",
    "\n",
    "print(f'Total time taken: {time.time() - start_time}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da13fce",
   "metadata": {},
   "source": [
    "The result using SON algorithm is similar to the result using Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba22049f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((120, 124, 205), 243),\n",
       " ((120, 124, 581), 240),\n",
       " ((124, 205, 834), 244),\n",
       " ((124, 581, 834), 244),\n",
       " ((262, 294, 853), 347)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemset3.take(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
