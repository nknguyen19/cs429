{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Read and parse the initial dataset\n",
    "\n",
    "# The raw data is currently stored in text file. \n",
    "# You need to store this raw data in an RDD, with each element of the RDD representing a data point as a comma-delimited string. \n",
    "# Each string starts with the label (a year) followed by numerical audio features. \n",
    "# Then check how many data points you have and print out a list of the first 5 data points.\n",
    "# Choose the number of partitions is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/07 13:59:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.app.name', 'Join'),\n",
       " ('spark.master', 'spark://master:7077'),\n",
       " ('spark.driver.host', 'master'),\n",
       " ('spark.app.startTime', '1673099968903'),\n",
       " ('spark.driver.port', '38083'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.id', 'app-20230107135930-0002'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.submitTime', '1673099968747'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setMaster('spark://master:7077').setAppName('Join')\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel('ERROR')\n",
    "sc.version\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1a) Firstly, copy the file YearPredictionMSD.txt from remote server to HDFS\n",
    "# Then, load data and check\n",
    "import os.path\n",
    "from itertools import islice\n",
    "baseDir = os.path.join('data')\n",
    "inputPath = os.path.join('Windsor_Housing_large.csv')\n",
    "fileName = os.path.join(baseDir, inputPath)\n",
    "\n",
    "numPartitions = 8\n",
    "\n",
    "rawData = sc.textFile(fileName, numPartitions)\\\n",
    "            .mapPartitionsWithIndex(\n",
    "                lambda index, it: islice(it, 1, None) if index == 0 else it # remove header line\n",
    "            )\\\n",
    "            .map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2546\n",
      "[['0', '42000.0', '5850', '3', '1', '2', 'yes', 'no', 'yes', 'no', 'no', '1', 'no'], ['1', '38500.0', '4000', '2', '1', '1', 'yes', 'no', 'no', 'no', 'no', '0', 'no'], ['2', '49500.0', '3060', '3', '1', '1', 'yes', 'no', 'no', 'no', 'no', '0', 'no'], ['3', '60500.0', '6650', '3', '1', '2', 'yes', 'yes', 'no', 'no', 'no', '0', 'no'], ['4', '61000.0', '6360', '2', '1', '1', 'yes', 'no', 'no', 'no', 'no', '0', 'no']]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "numPoints = rawData.count()\n",
    "print(numPoints)\n",
    "samplePoints = rawData.take(5)\n",
    "print(samplePoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load testing library\n",
    "#from test_helper import assertEquals\n",
    "# I cannot import test_helper library\n",
    "def assertEquals(src, target, msg):\n",
    "    assert src == target, msg\n",
    "\n",
    "def assertTrue(src, msg):\n",
    "    assert src, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1b) Using LabeledPoint\n",
    "# LabeledPoint is an object of the library pyspark.mllib.regression.\n",
    "# You need to use it to parse the features and label for the list of data point from the previous question.\n",
    "# Write a function that takes an input as a raw data point, parses it by using Python's unicode.split method and returns a LabeledPoint. \n",
    "# Then print out the features and label for the point. Finally, calculate the number features for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0,5850.0,3.0,1.0,2.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0] 42000.0\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def parsePoint(line):\n",
    "    \"\"\"Converts a comma separated unicode string into a `LabeledPoint`.\n",
    "\n",
    "    Args:\n",
    "        line (unicode): Comma separated unicode string where the first element is the label and the remaining elements are features.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: The line is converted into a `LabeledPoint`, which consists of a label and features.\n",
    "    \"\"\"\n",
    "    return LabeledPoint(line[1], [1] + [1 if x in ['\"yes\"', 'yes'] else 0 if x in ['\"no\"', 'no'] else x for x in line[2:]])\n",
    "\n",
    "parsedSamplePoints = [parsePoint(point) for point in samplePoints]\n",
    "firstPointFeatures = parsedSamplePoints[0].features\n",
    "firstPointLabel = parsedSamplePoints[0].label\n",
    "print(firstPointFeatures, firstPointLabel)\n",
    "\n",
    "d = len(firstPointFeatures)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1c) Find the range\n",
    "# Let's examine the labels to find the range of song years. \n",
    "# To do this, first parse each element of the rawData RDD, and then find the smallest and largest labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15200.0] [1988100.0]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "parsedDataInit = rawData.map(lambda x: parsePoint(x))\n",
    "onlyLabels = parsedDataInit.map(lambda x: x.label)\n",
    "minPrice = onlyLabels.takeOrdered(1)\n",
    "maxPrice = onlyLabels.takeOrdered(1, lambda x: -x)\n",
    "print(minPrice, maxPrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1d) Shift labels \n",
    "# The labels are years in the 1900s and 2000s.\n",
    "# We should shift them such that they start from zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.mllib.regression.LabeledPoint'>\n",
      "\n",
      "[LabeledPoint(42000.0, [1.0,5850.0,3.0,1.0,2.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0])]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "parsedData = parsedDataInit\n",
    "\n",
    "# Should be a LabeledPoint\n",
    "print(type(parsedData.take(1)[0]))\n",
    "# View the first point\n",
    "print('\\n{0}'.format(parsedData.take(1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1e) Training, validation, and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013 274 259 2546\n",
      "2546\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Firstly, we will split the dataset into training, validation and test sets.\n",
    "# Use the randomSplit method with the specified weights and seed to create RDDs storing each of these datasets.\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "parsedTrainData, parsedValData, parsedTestData = parsedData.randomSplit(weights, seed)\n",
    "\n",
    "# Then, we cache each of these RDDs, as we will be accessing them multiple times.\n",
    "parsedTrainData.cache()\n",
    "parsedValData.cache()\n",
    "parsedTestData.cache()\n",
    "\n",
    "# Finally, compute the size of each dataset and verify the sum of three sizes.\n",
    "nTrain = parsedTrainData.count()\n",
    "nVal = parsedValData.count()\n",
    "nTest = parsedTestData.count()\n",
    "\n",
    "print(nTrain, nVal, nTest, nTrain + nVal + nTest)\n",
    "print(parsedData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2b) Root mean squared error \n",
    "# We want to know how well the naive baseline performs.\n",
    "# Therefore, we will use root mean squared error (RMSE) for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def squaredError(label, prediction):\n",
    "    \"\"\"Calculates the the squared error for a single prediction.\n",
    "\n",
    "    Args:\n",
    "        label (float): The correct value for this observation.\n",
    "        prediction (float): The predicted value for this observation.\n",
    "\n",
    "    Returns:\n",
    "        float: The difference between the `label` and `prediction` squared.\n",
    "    \"\"\"\n",
    "    return (label - prediction) ** 2\n",
    "\n",
    "def calcRMSE(labelsAndPreds):\n",
    "    \"\"\"Calculates the root mean squared error for an `RDD` of (label, prediction) tuples.\n",
    "\n",
    "    Args:\n",
    "        labelsAndPred (RDD of (float, float)): An `RDD` consisting of (label, prediction) tuples.\n",
    "\n",
    "    Returns:\n",
    "        float: The square root of the mean of the squared errors.\n",
    "    \"\"\"\n",
    "    squared_error = labelsAndPreds.map(lambda x: squaredError(x[0], x[1])).sum() / labelsAndPreds.count()\n",
    "    return math.sqrt(squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2909944487358056\n"
     ]
    }
   ],
   "source": [
    "# Example with a small RDD of (label, prediction) tuples\n",
    "labelsAndPreds = sc.parallelize([(3., 1.), (1., 2.), (2., 2.)])\n",
    "# RMSE = sqrt[((3-1)^2 + (1-2)^2 + (2-2)^2) / 3] = 1.291\n",
    "exampleRMSE = calcRMSE(labelsAndPreds)\n",
    "print(exampleRMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: Root mean squared error \n",
    "assertTrue(np.allclose(squaredError(3, 1), 4.), 'incorrect definition of squaredError')\n",
    "assertTrue(np.allclose(exampleRMSE, 1.2909944487358056), 'incorrect value for exampleRMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Train (via gradient descent) and evaluate a linear regression model\n",
    "\n",
    "# We can do a better prediction via linear regression, by using gradient descent (omit the intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3a) Gradient descent\n",
    "\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# First, we implement a function that computes the summand for the update, i.e. (w⊤x−y)x.\n",
    "def gradientSummand(weights, lp):\n",
    "    \"\"\"Calculates the gradient summand for a given weight and `LabeledPoint`.\n",
    "\n",
    "    Note:\n",
    "        `DenseVector` behaves similarly to a `numpy.ndarray` and they can be used interchangably\n",
    "        within this function.  For example, they both implement the `dot` method.\n",
    "\n",
    "    Args:\n",
    "        weights (DenseVector): An array of model weights (betas).\n",
    "        lp (LabeledPoint): The `LabeledPoint` for a single observation.\n",
    "\n",
    "    Returns:\n",
    "        DenseVector: An array of values the same length as `weights`.  The gradient summand.\n",
    "    \"\"\"\n",
    "    return (weights.dot(lp.features.toArray()) - lp.label) * lp.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18.0,6.0,24.0]\n",
      "[1.7304000000000002,-5.191200000000001,-2.5956000000000006]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "# And then, test out this function.\n",
    "exampleW = DenseVector([1, 1, 1])\n",
    "exampleLP = LabeledPoint(2.0, [3, 1, 4])\n",
    "# gradientSummand = (dot([1 1 1], [3 1 4]) - 2) * [3 1 4] = (8 - 2) * [3 1 4] = [18 6 24]\n",
    "summandOne = gradientSummand(exampleW, exampleLP)\n",
    "print(summandOne)\n",
    "\n",
    "exampleW = DenseVector([.24, 1.2, -1.4])\n",
    "exampleLP = LabeledPoint(3.0, [-1.4, 4.2, 2.1])\n",
    "summandTwo = gradientSummand(exampleW, exampleLP)\n",
    "print(summandTwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: Gradient summand\n",
    "assertTrue(np.allclose(summandOne, [18., 6., 24.]), 'incorrect value for summandOne')\n",
    "assertTrue(np.allclose(summandTwo, [1.7304, -5.1912, -2.5956]), 'incorrect value for summandTwo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3b) Use weights to make predictions\n",
    "\n",
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# We will implement a getLabeledPredictions function that computes the dot product between weights and observation's features.\n",
    "# The function returns a (label, prediction) tuple.\n",
    "def getLabeledPrediction(weights, observation):\n",
    "    \"\"\"Calculates predictions and returns a (label, prediction) tuple.\n",
    "\n",
    "    Note:\n",
    "        The labels should remain unchanged as we'll use this information to calculate prediction\n",
    "        error later.\n",
    "\n",
    "    Args:\n",
    "        weights (np.ndarray): An array with one weight for each features in `trainData`.\n",
    "        observation (LabeledPoint): A `LabeledPoint` that contain the correct label and the\n",
    "            features for the data point.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A (label, prediction) tuple.\n",
    "    \"\"\"\n",
    "    return (observation.label, observation.features.dot(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2.0, 1.75), (1.5, 1.25)]\n"
     ]
    }
   ],
   "source": [
    "# Test out the function with a small example\n",
    "weights = np.array([1.0, 1.5])\n",
    "predictionExample = sc.parallelize([LabeledPoint(2, np.array([1.0, .5])),\n",
    "                                    LabeledPoint(1.5, np.array([.5, .5]))])\n",
    "labelsAndPredsExample = predictionExample.map(lambda lp: getLabeledPrediction(weights, lp))\n",
    "print(labelsAndPredsExample.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: Use weights to make predictions \n",
    "assertEquals(labelsAndPredsExample.collect(), [(2.0, 1.75), (1.5, 1.25)], 'incorrect definition for getLabeledPredictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3c) Gradient descent \n",
    "# We will implement a gradient descent function for linear regression \n",
    "# And then test out this function on an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def linregGradientDescent(trainData, numIters):\n",
    "    \"\"\"Calculates the weights and error for a linear regression model trained with gradient descent.\n",
    "\n",
    "    Note:\n",
    "        `DenseVector` behaves similarly to a `numpy.ndarray` and they can be used interchangably\n",
    "        within this function.  For example, they both implement the `dot` method.\n",
    "\n",
    "    Args:\n",
    "        trainData (RDD of LabeledPoint): The labeled data for use in training the model.\n",
    "        numIters (int): The number of iterations of gradient descent to perform.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray): A tuple of (weights, training errors).  Weights will be the\n",
    "            final weights (one weight per feature) for the model, and training errors will contain\n",
    "            an error (RMSE) for each iteration of the algorithm.\n",
    "    \"\"\"\n",
    "    # The length of the training data\n",
    "    n = trainData.count()\n",
    "    # The number of features in the training data\n",
    "    d = len(trainData.take(1)[0].features)\n",
    "    w = np.zeros(d)\n",
    "    alpha = 0.00001\n",
    "    # We will compute and store the training error after each iteration\n",
    "    errorTrain = np.zeros(numIters)\n",
    "    for i in range(numIters):\n",
    "        # Use getLabeledPrediction from (3b) with trainData to obtain an RDD of (label, prediction) tuples. \n",
    "        # Note that the weights all equal 0 for the first iteration, so the predictions will have large errors to start.\n",
    "        labelsAndPredsTrain = trainData.map(lambda x: getLabeledPrediction(w, x))\n",
    "        errorTrain[i] = calcRMSE(labelsAndPredsTrain)\n",
    "\n",
    "        # Calculate the `gradient`.  Make use of the `gradientSummand` function you wrote in (3a).\n",
    "        # Note that `gradient` sould be a `DenseVector` of length `d`.\n",
    "        gradient = trainData.map(lambda x: gradientSummand(w, x)).sum() / n\n",
    "        # Update the weights\n",
    "        alpha_i = alpha / (n * np.sqrt(i+1))\n",
    "#         print(labelsAndPredsTrain.take(1))\n",
    "#         print(gradient, alpha_i)\n",
    "        w -= alpha_i * gradient\n",
    "#         print(w)\n",
    "    return w, errorTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(42000.0, [1.0,5850.0,3.0]), LabeledPoint(38500.0, [1.0,4000.0,2.0])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.06533317e+03 1.10767552e+07 5.76909680e+03]\n",
      "[6.45547752e+04 1.60214545e+06 2.88484419e+07 4.18861965e+08\n",
      " 5.21072415e+09]\n"
     ]
    }
   ],
   "source": [
    "# Create a toy dataset with n = 10, d = 3, and then run 5 iterations of gradient descent.\n",
    "# The resulting model will not be useful.\n",
    "# The goal here is to verify that linregGradientDescent is working properly.\n",
    "exampleN = 10\n",
    "exampleD = 3\n",
    "exampleData = (sc\n",
    "               .parallelize(parsedTrainData.take(exampleN))\n",
    "               .map(lambda lp: LabeledPoint(lp.label, lp.features[0:exampleD])))\n",
    "print(exampleData.take(2))\n",
    "exampleNumIters = 5\n",
    "exampleWeights, exampleErrorTrain = linregGradientDescent(exampleData, exampleNumIters)\n",
    "print(exampleWeights)\n",
    "print(exampleErrorTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3d) Train the model\n",
    "# Let's train a linear regression model on all of our training data.\n",
    "# Then evaluate its accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE:\n",
      "\tBaseline = 0.000\n",
      "\tLR0 = 193716.707\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "numIters = 50\n",
    "weightsLR0, errorTrainLR0 = linregGradientDescent(parsedData, numIters)\n",
    "\n",
    "labelsAndPreds = parsedValData.map(lambda x: getLabeledPrediction(weightsLR0, x))\n",
    "rmseValLR0 = calcRMSE(labelsAndPreds)\n",
    "\n",
    "print('Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 = {1:.3f}'.format(0, rmseValLR0)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
